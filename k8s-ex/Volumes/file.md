#### Data in Container level -> First Commit - Data is saved in container, and if that container inside the pod is deleted, all the data is removed. We created a deployment with 1 pod, inside of that pod was just 1 mongodb container. We created a service to expose that pod in a port. We used localhost:32000 to connect with this pod using mongo compass. In mongo compass we created a db and a record inside it, that means that inside the pod was this db and record. Then we went inside the pod using "exec -it ... -- /bin/bash" and killed mongodb process, so we destroyed that container. We went to the mongo compass and saw that the db and the record was deleted too.

### Data in Pod level -> Solution of that above-> store the data in pod level, use #emptyDir#. Data is stored as the pod is running. Even if the container is restarted, the data is stored in the pod. If multiple containers, we can mount those containers to the same volume, because volume is created in pod level, and we use it to the container.  Now if we create that deployment, and the service and go to mongo compass using the localhost:expose_port, create a db and a record, then go inside the pod and kill the container, then we can go to mongo compass and see that the db and record is not deleted. This is all beacuse we created a volume in pod level that container can use.
### This is not solution(emptyDir is not perfect solution). If pod is down and deleted and created from deployment all the data will be lost. Other pods cannot use the data. If we delete the pod, then the deployment will replace with another, but if we go to mongo compass, we see that the db and the record is deleted because that was in a pod level and that pod is gone.Solution-> take volume out of the pod(hostPath, all pods in same node can share the data)

### The solution as we talked is #hostPath# instead of emptyDir, and we specify a path in the node where the data will be stored so it can be used from all the pods in that node, and it will be mounted in the container in /data/db path. Buttt the data is not shared among nodes, so node 2 vannot be used data in node 1. Solution-> take the storage from the node and move it to external storage like aws(Persisent Volumes). The volumes that we used till now was ephemeral volumes, so when a pod or node gets deleted the data associated with those volumes also gets deleted

# Persistent VOLUMES
#### We define as K8S components in a yaml file. It is a kubernetes abstract component and it must take storage from the actual physical storage  like AWS-WBS etc. We deifne the capacity-storage in Gi. Then we define the accessMode. We have: ReadWriteMany(read&write from many nodes), ReadWriteOnce(read&write from single nodes), ReadOnlyOnce, ReadOnlyMany, ReadWriteOncePod
### Okay, we created the Persistent Volumes, but how to use this Volumes. Thats when the PersistentVolumeClaim joins the game, it is also a k8s component that is declared in a yaml file. We mention how much storage our pod needs. So we create this PVC, then we specify it to the deployment in pod level to the volumes: . After that the pod when need storage will go to this PVC and attaches the PersistentVolume that is bound to the PVC.

